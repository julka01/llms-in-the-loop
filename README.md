# LLMs in the loop: Leveraging Large Language Model annotations for Active Learning in low-resource languages

## Project Structure

Below is the tree structure of the project along with a description of each module:

```

â”‚
â”œâ”€â”€ data # Directory for saving data used in the notebooks
â”‚
â”œâ”€â”€ Experiments # Directory for running and saving experiments
â”‚  â”œâ”€â”€ active_learning_on_foundation_model # Notebooks related to each language for re-training initial model using activate learning based on uncertainty sampling
â”‚  â”œâ”€â”€ active_learning_on_ground_truth # Notebooks related to each language for re-training initial model using activate learning based on human annotation
â”‚  â”œâ”€â”€ foundation_model_selection # Notebooks related to each large language model and experiment related to that model
â”‚  â””â”€â”€ simple_training_on_ground_truth # Notebooks related to each language for simple model training without using activate learning
â”‚
â”œâ”€â”€ notebooks # Jupyter notebooks
â”‚  â”œâ”€â”€ 00-token_counts.ipynb # Notebook for analyzing token counts in the dataset
â”‚  â”œâ”€â”€ 01-preprocessing.ipynb # Notebook for data preprocessing steps
â”‚  â”œâ”€â”€ 02-querying.ipynb # Notebook for querying foundation model
â”‚  â”œâ”€â”€ 04.1-sampling_for_foundation_model_selection.ipynb 
â”‚  â””â”€â”€ 04.2-sampling_pipeline.ipynb # Example notebook how to use sampling function
â”‚
â”œâ”€â”€ settings # Contains configuration files for the project
â”‚  â””â”€â”€ config.yml # YAML configuration file with overall project settings
â”‚
â”œâ”€â”€ src # Source code for the project
â”‚  â”œâ”€â”€ data # Scripts related to data handling
â”‚  â”‚  â”œâ”€â”€ preprocess.py # Python script for preprocessing data
â”‚  â”‚  â””â”€â”€ sample.py # Functions for sampling the data for the foundation model selection
â”‚  â”‚
â”‚  â”œâ”€â”€ models # Scripts for defining models
â”‚  â”‚  â””â”€â”€ xlmr_ner.py # Script for an XLM-R model for Named Entity Recognition
â”‚  â”‚
â”‚  â”œâ”€â”€ query # Scripts for querying foundation models
â”‚  â”‚  â”œâ”€â”€ annotation_examples.json # Manually created annotation examples
â”‚  â”‚  â”œâ”€â”€ ner_examples_all_languages.json # Automatically created annotation examples
â”‚  â”‚  â”œâ”€â”€ generate_annotation_examples.ipynb # Notebook to generate annotation examples
â”‚  â”‚  â”œâ”€â”€ prompts.py # Prompts for querying foundation models
â”‚  â”‚  â””â”€â”€ query_gpt.py # Script for querying foundation models
â”‚  â”‚
â”‚  â””â”€â”€ utils # Utility scripts used across the project
â”‚     â””â”€â”€ utils.py # General utility functions
â”‚
â”œâ”€â”€ README.md # The file you are currently reading
â”‚
â”œâ”€â”€ .gitignore # Ignore special files from commiting to the git repository
â”‚
â””â”€â”€ requirements.txt # Required libraries for the project to run
```

## Usage

### ðŸ“’ Notebooks

Folder `notebook` contains the main logic of the project.

* To explore text preprocessing steps, see `notebooks/01-preprocessing.ipynb`


* To explore how we query foundation model, see `notebooks/02-querying.ipynb`


* To explore how to use the sampling function for foundation model selection, see `notebooks/04.1-sampling_pipeline.ipynb`.
For more details, see `04.1-sampling_for_foundation_model_selection.ipynb`.

### ðŸ§ª Experiments

Folder `experiments` contains various experiment runs with different settings.

### ðŸ“„ Paper

A preprint version of the paper related to this repository is available at arXiv: [https://arxiv.org/abs/2404.02261](https://arxiv.org/abs/2404.02261)

This paper has been accepted for publication at ECML PKDD 2024.
